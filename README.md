# Attention Mechanism from Scratch

## Overview
This repository contains an implementation of the Attention Mechanism from scratch using Python. Attention is a critical component of many modern neural network architectures, such as Transformers, which have become the backbone of models like GPT, BERT, and other advanced AI systems. This implementation aims to help you understand the fundamental concepts of the attention mechanism by providing a step-by-step guide and the complete code.

## Features
- Implementation of the basic attention mechanism from scratch.
- Clear and well-documented code to facilitate learning.

## Requirements
- Python 3.x
- NumPy
- Jupyter Notebook
- PyTorch

You can install the required packages via:
```bash
pip install numpy jupyter torch
```

To install PyTorch, refer to the official installation guide at: https://pytorch.org/get-started/locally/


## How to Run
1. Clone the repository:
    ```bash
    git clone https://github.com/NKSTUD/attention_mechanisme_from_scratch.git
    ```

2. Navigate to the project directory:
    ```bash
    cd attention_mechanisme_from_scratch
    ```

3. Run the Jupyter Notebook:
    ```bash
    jupyter notebook attention_mechanism.ipynb
    ```


